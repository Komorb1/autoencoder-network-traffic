{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading UNSW-NB15 dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "trainPATH = './UNSW_NB15/train.csv'\n",
    "testPATH = './UNSW_NB15/test.csv'\n",
    "\n",
    "trainDf = pd.read_csv(trainPATH)\n",
    "testDf = pd.read_csv(testPATH)\n",
    "\n",
    "trainDf = trainDf.drop(columns=['attack_cat'])\n",
    "testDf = testDf.drop(columns=['attack_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train:\",trainDf.shape)\n",
    "print(\"Test:\",testDf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799da4ca",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "- handling missing values\n",
    "- outlier detection \n",
    "- encoding categorical variables\n",
    "- balancing classes\n",
    "- normalization or standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e16ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. handling missing values\n",
    "print(\"Missing in train:\\n\", trainDf.isnull().sum())\n",
    "print(\"Missing in test:\\n\", testDf.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a546997",
   "metadata": {},
   "source": [
    "No missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1221492",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = trainDf.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = trainDf.select_dtypes(include=['object']).columns\n",
    "\n",
    "test_num_cols = testDf.select_dtypes(include=['int64', 'float64']).columns\n",
    "test_cat_cols = testDf.select_dtypes(include=['object']).columns\n",
    "\n",
    "print('TrainDf numeric columns:',num_cols,'\\n\\nTrainDf categorical columns:',cat_cols)\n",
    "print('\\n TestDf numeric columns:',test_num_cols,'\\n\\nTestDf categorical columns:',test_cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ba257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. outlier detection and treatment\n",
    "\n",
    "# outlier detection\n",
    "def outlier_det(df, cols):\n",
    "    outliers = {}\n",
    "\n",
    "    for col in cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "\n",
    "        outlier = df[(df[col] < lower) | (df[col] > upper)]\n",
    "        outliers[col] = len(outlier) \n",
    "\n",
    "    outliers = dict(sorted(outliers.items(), key= lambda x: x[1], reverse=True))\n",
    "\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba989fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using IQR method to detect outliers gave the following \\n Column: outlier amount\")\n",
    "print('TrainDf: ',outlier_det(trainDf,num_cols))\n",
    "print('TestDf: ',outlier_det(testDf, test_num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c36541",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_percent = {\n",
    "    col: f'{round((count / len(trainDf)) * 100, 2)}%'\n",
    "    for col, count in outlier_det(trainDf, num_cols).items()\n",
    "}\n",
    "\n",
    "test_outlier_percent = {\n",
    "    col: f'{round((count / len(testDf)) * 100, 2)}%'\n",
    "    for col, count in outlier_det(testDf, test_num_cols).items()\n",
    "}\n",
    "\n",
    "print('Outlier percentages for each column','\\n Column: outlier percentage')\n",
    "print('TrainDf:', outlier_percent)\n",
    "print('TestDf: ', test_outlier_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b37429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved original before outlier treatment for plotting\n",
    "trainDf_original = trainDf.copy()\n",
    "testDf_original = testDf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41214c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier treatment\n",
    "\n",
    "def outlier_treat(df, cols):\n",
    "    for col in cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "\n",
    "        df[col] = np.where(df[col]< lower, lower, np.where(df[col] > upper, upper, df[col])) \n",
    "\n",
    "    return df\n",
    "\n",
    "trainDf = outlier_treat(trainDf, num_cols)\n",
    "\n",
    "print('After outlier treatment\\nColumn: outlier')\n",
    "print('TrainDf: ',outlier_det(trainDf, num_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Encoding of categorical variables\n",
    "\n",
    "# separating features from labels\n",
    "x_train = trainDf.drop(columns=['label'])\n",
    "y_train = trainDf['label']\n",
    "\n",
    "x_test = testDf.drop(columns=['label'])\n",
    "y_test = testDf['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be012d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Applying OneHotEncoder on train\n",
    "ohe = OneHotEncoder(handle_unknown = 'ignore', sparse_output = False)\n",
    "ohe.fit(x_train[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ohe = ohe.transform(x_train[cat_cols])\n",
    "x_test_ohe  = ohe.transform(x_test[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aab081",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols = ohe.get_feature_names_out(cat_cols)\n",
    "\n",
    "x_train_ohe = pd.DataFrame(x_train_ohe, columns=ohe_cols, index=x_train.index)\n",
    "x_test_ohe  = pd.DataFrame(x_test_ohe,  columns=ohe_cols, index=x_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_num = x_train.drop(columns=cat_cols)\n",
    "x_test_num  = x_test.drop(columns=cat_cols)\n",
    "\n",
    "# Combine numerical + encoded categorical features\n",
    "x_train_encoded = pd.concat([x_train_num, x_train_ohe], axis=1)\n",
    "x_test_encoded  = pd.concat([x_test_num,  x_test_ohe], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoded train shape:\", x_train_encoded.shape)\n",
    "print(\"Encoded test shape:\", x_test_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88108c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Balancing classes on training set using SMOTE\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "x_train_res, y_train_res = sm.fit_resample(x_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before SMOTE:', y_train.value_counts())\n",
    "print('\\nAfter SMOTE:', y_train_res.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e28bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Normalization or Standardization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# scale training set only\n",
    "x_train_scaled = scaler.fit_transform(x_train_res)\n",
    "# transform test set\n",
    "x_test_scaled = scaler.transform(x_test_encoded)\n",
    "\n",
    "print(\"Train scaled shape:\", x_train_scaled.shape)\n",
    "print(\"Test scaled shape:\", x_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072f367",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA)\n",
    "-\tCompute summary statistics (mean, median, variance, distribution, etc.)\n",
    "-\tPerform visualizations (histograms, box plots, correlation heatmaps, scatter plots, etc.)\n",
    "-\tExamine relationships between variables and the target feature\n",
    "-\tPresent observations and insights from data patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84cbe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "\n",
    "trainDf_original.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93760f40",
   "metadata": {},
   "source": [
    "### Summary Statistics of Numerical Features\n",
    "\n",
    "The table above provides summary statistics (count, mean, standard deviation, min, quartiles, max) for all numerical features in the training dataset. These statistics help identify data distribution properties, potential skewness, and unusually large ranges that may indicate outliers.\n",
    "\n",
    "**Key observations:**\n",
    "- Several features (e.g., `sbytes`, `dbytes`, `dur`) show very large max values relative to their medians, suggesting heavy right-skewed distributions.\n",
    "- Network traffic features often have long tails due to occasional large flows, which aligns with the observed results.\n",
    "- The large standard deviations in several columns confirm the presence of extreme values, motivating the need for outlier detection and treatment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_original[cat_cols].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da7a0e",
   "metadata": {},
   "source": [
    "### Summary Statistics of Categorical Features\n",
    "\n",
    "This table summarizes the categorical variables in the dataset, including the number of unique categories and the most frequent category for each feature.\n",
    "\n",
    "**Insights:**\n",
    "- The `proto` column is dominated by common protocols such as TCP and UDP.\n",
    "- The `service` column contains many unique values due to a diverse set of network services.\n",
    "- The `state` column indicates connection state transitions. Some states are significantly more frequent, reflecting typical network traffic patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6bd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations (histograms, box plots, correlation heatmaps, scatter plots, etc.)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x='label', data=trainDf_original)\n",
    "plt.title(\"Normal vs Malicious Distribution (Training Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dd1633",
   "metadata": {},
   "source": [
    "### Class Distribution (Normal vs Malicious)\n",
    "\n",
    "This count plot illustrates the distribution of the target variable (`label`) in the training dataset.\n",
    "\n",
    "**Observations:**\n",
    "- The dataset is imbalanced, with more malicious samples (label = 1) than normal samples (label = 0).\n",
    "- This imbalance can negatively affect model performance, making class balancing (e.g., SMOTE) necessary.\n",
    "- The imbalance also reflects the UNSW-NB15 dataset design, where the training split intentionally includes more attack traffic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_original[num_cols].hist(figsize=(18, 15), bins=50, color='skyblue')\n",
    "plt.suptitle(\"Numerical Feature Distributions (Histogram)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f3737",
   "metadata": {},
   "source": [
    "### Histograms of Numerical Features\n",
    "\n",
    "The histograms above show the distribution of all numerical features. These plots help visualize skewness, spread, and feature ranges.\n",
    "\n",
    "**Insights:**\n",
    "- Many features exhibit heavy right-skew, meaning most values are small, with a few extremely large values.\n",
    "- Features such as `sbytes`, `dbytes`, and `dur` show long-tailed distributions, common in network traffic datasets.\n",
    "- The presence of long tails supports the need for outlier treatment using the IQR method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a11ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "trainDf_original[num_cols].boxplot()\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Boxplots of Numerical Features (Raw Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856bbc3",
   "metadata": {},
   "source": [
    "### Boxplot of Numerical Features (Before Outlier Treatment)\n",
    "\n",
    "The boxplot visualizes the spread and outliers for each numerical feature.\n",
    "\n",
    "**Insights:**\n",
    "- Nearly all features contain extreme outliers, visible as distant points beyond whiskers.\n",
    "- Outliers are typical in cybersecurity data due to high-traffic anomalies and rare attack patterns.\n",
    "- This plot validates the statistical findings and demonstrates why outlier treatment is essential before model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ed9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 12))\n",
    "corr = trainDf_original[num_cols].corr()\n",
    "sns.heatmap(corr, cmap='coolwarm', annot=False)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b5c52e",
   "metadata": {},
   "source": [
    "### Correlation Heatmap\n",
    "\n",
    "This heatmap visualizes pairwise correlations between numerical features.\n",
    "\n",
    "**Key observations:**\n",
    "- Several packet- and byte-related features (e.g., `sbytes`, `spkts`, `dbytes`, `dpkts`) show moderate to strong correlations.\n",
    "- Features such as TTL-based values (`sttl`, `dttl`) also correlate with each other.\n",
    "- Most features show low correlation, suggesting that the dataset does not suffer from excessive multicollinearity, which is beneficial for training models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.boxplot(x='label', y='sbytes', data=trainDf_original)\n",
    "plt.title(\"sbytes vs Label\")\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.boxplot(x='label', y='dbytes', data=trainDf_original)\n",
    "plt.title(\"dbytes vs Label\")\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.boxplot(x='label', y='dur', data=trainDf_original)\n",
    "plt.title(\"dur vs Label\")\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.boxplot(x='label', y='ct_state_ttl', data=trainDf_original)\n",
    "plt.title(\"ct_state_ttl vs Label\")\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.boxplot(x='label', y='rate', data=trainDf_original)\n",
    "plt.title(\"rate vs Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dfa7df",
   "metadata": {},
   "source": [
    "### Numerical Feature Comparison (Benign vs Malicious)\n",
    "\n",
    "These boxplots compare selected numerical features across benign (label 0) and malicious (label 1) traffic.\n",
    "\n",
    "**Patterns observed:**\n",
    "- **`sbytes` and `dbytes`**: Malicious flows tend to have higher byte counts compared to benign traffic.\n",
    "- **`dur`**: Attack traffic shows greater variance in flow duration.\n",
    "- **`ct_state_ttl`**: This aggregated state/TTL feature shows clear differences between classes, making it potentially useful for classification.\n",
    "- **`rate`**: Malicious traffic often has higher connection rates, indicative of scanning or DoS behavior.\n",
    "\n",
    "These patterns indicate strong predictive power for these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754421fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(x='sbytes', y='dbytes', hue='label', data=trainDf_original, alpha=0.4)\n",
    "plt.title(\"Scatter Plot of sbytes vs dbytes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ad361",
   "metadata": {},
   "source": [
    "### Scatter Plot of sbytes vs dbytes\n",
    "\n",
    "This scatter plot shows the relationship between source bytes (`sbytes`) and destination bytes (`dbytes`), colored by class label.\n",
    "\n",
    "**Insights:**\n",
    "- Benign traffic is clustered near the origin, with lower byte counts.\n",
    "- Malicious traffic shows more spread and includes many high-byte flows.\n",
    "- The clear separation in some regions indicates this feature pair can help distinguish between benign and malicious samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d83317",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_original['proto'].value_counts().plot(kind='bar', figsize=(10,5))\n",
    "plt.title(\"Protocol Types Frequency\")\n",
    "plt.show()\n",
    "\n",
    "trainDf_original['service'].value_counts().head(20).plot(kind='bar', figsize=(10,5))\n",
    "plt.title(\"Top 20 Services\")\n",
    "plt.show()\n",
    "\n",
    "trainDf_original['state'].value_counts().plot(kind='bar', figsize=(10,5))\n",
    "plt.title(\"Connection State Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b3a41",
   "metadata": {},
   "source": [
    "### Categorical Feature Distributions\n",
    "\n",
    "Bar plots are shown for protocol types, top 20 services, and connection states.\n",
    "\n",
    "**Insights:**\n",
    "- **Protocol distribution** is dominated by TCP and UDP, which aligns with real-world internet traffic.\n",
    "- **Service distribution** has a heavy tail with many rarely used services, some of which are used predominantly by attack traffic.\n",
    "- **State distribution** reveals which connection transitions occur most frequently, with some states highly correlated with malicious activity.\n",
    "\n",
    "Understanding these distributions helps interpret model behavior and feature importance later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223d1a7",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "All students must apply at least three of the following algorithms:\n",
    "1.\tDecision Tree\n",
    "2.\tNaive Bayes\n",
    "3.\tSupport Vector Machine (SVM)\n",
    "4.\tArtificial Neural Network (MLP or CNN depending on data type)\n",
    "5.\tRandom Forest (optional)\n",
    "6.\tLogistic Regression (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dt.fit(x_train_scaled, y_train_res)\n",
    "\n",
    "y_pred_dt = dt.predict(x_test_scaled)\n",
    "y_pred_prob_dt = dt.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "print(\"Decision Tree Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_prob_dt))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm_dt)\n",
    "\n",
    "# ROC Curve\n",
    "RocCurveDisplay.from_estimator(dt, x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "nb.fit(x_train_scaled, y_train_res)\n",
    "\n",
    "y_pred_nb = nb.predict(x_test_scaled)\n",
    "y_pred_prob_nb = nb.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "print(\"Naive Bayes Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_prob_nb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
    "\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm_nb)\n",
    "\n",
    "RocCurveDisplay.from_estimator(nb, x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372bde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "rf.fit(x_train_res, y_train_res)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf.predict(x_test_encoded)\n",
    "y_pred_prob_rf = rf.predict_proba(x_test_encoded)[:, 1]\n",
    "\n",
    "print(\"Random Forest Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_prob_rf))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "RocCurveDisplay.from_estimator(rf, x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross-Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_cv = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5-fold cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    rf_cv,\n",
    "    x_train_encoded,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "print(\"5-Fold CV F1 Scores:\", cv_scores)\n",
    "print(\"Mean F1:\", cv_scores.mean())\n",
    "print(\"Std Dev:\", cv_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447098d5",
   "metadata": {},
   "source": [
    "### What this does:\n",
    "\n",
    "- Splits the training set into 5 folds\n",
    "\n",
    "- Re-trains and evaluates 5 times\n",
    "\n",
    "- Computes average F1 score\n",
    "\n",
    "- Shows stability of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de754757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [20, 40, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid=rf_params,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid.fit(x_train_encoded, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", rf_grid.best_params_)\n",
    "best_rf = rf_grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41838141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tuned model\n",
    "best_rf.fit(x_train_res, y_train_res)\n",
    "\n",
    "# Predict using tuned model\n",
    "y_pred_rf_best = best_rf.predict(x_test_encoded)\n",
    "y_pred_prob_rf_best = best_rf.predict_proba(x_test_encoded)[:, 1]\n",
    "\n",
    "print(\"Random Forest (Tuned) Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf_best))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_prob_rf_best))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf_best))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf_best))\n",
    "\n",
    "# ROC Curve\n",
    "RocCurveDisplay.from_estimator(best_rf, x_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f1e9a",
   "metadata": {},
   "source": [
    "Parameter tuning was performed using GridSearchCV with 3-fold cross-validation. <br>\n",
    "We optimized hyperparameters such as n_estimators, max_depth, min_samples_split, and min_samples_leaf to improve model performance. <br>\n",
    "The best model found by the grid search was used for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1acce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    "\n",
    "def evaluate_model(model_name, y_true, y_pred, y_prob):\n",
    "    print(f\"\\n      {model_name} Evaluation      \")\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy:      {acc:.4f}\")\n",
    "    print(f\"Precision:     {prec:.4f}\")\n",
    "    print(f\"Recall:        {rec:.4f}\")\n",
    "    print(f\"F1-score:      {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:       {roc:.4f}\")\n",
    "    print(f\"MCC:           {mcc:.4f}\")\n",
    "    print(f\"Cohen’s Kappa: {kappa:.4f}\")\n",
    "    print(f\"Balanced Acc:  {bal_acc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-score\": f1,\n",
    "        \"ROC-AUC\": roc,\n",
    "        \"MCC\": mcc,\n",
    "        \"Kappa\": kappa,\n",
    "        \"Balanced Acc\": bal_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ab1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dt = evaluate_model(\n",
    "    \"Decision Tree\",\n",
    "    y_test,\n",
    "    y_pred_dt,\n",
    "    y_pred_prob_dt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nb = evaluate_model(\n",
    "    \"Naive Bayes\",\n",
    "    y_test,\n",
    "    y_pred_nb,\n",
    "    y_pred_prob_nb\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf = evaluate_model(\n",
    "    \"Random Forest\",\n",
    "    y_test,\n",
    "    y_pred_rf,\n",
    "    y_pred_prob_rf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a69db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tuned_rf = evaluate_model(\n",
    "    \"Tuned Random Forest\",\n",
    "    y_test,\n",
    "    y_pred_rf_best,\n",
    "    y_pred_prob_rf_best\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_prob_dt, name=\"Decision Tree\")\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_prob_nb, name=\"Naive Bayes\")\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_prob_rf, name=\"Random Forest\")\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_prob_rf_best, name=\"Tuned Random Forest\")\n",
    "\n",
    "plt.title(\"ROC Curves for All Models\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.DataFrame([\n",
    "    results_dt,\n",
    "    results_nb,\n",
    "    results_rf,\n",
    "    results_tuned_rf\n",
    "])\n",
    "\n",
    "evaluation_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3104f38",
   "metadata": {},
   "source": [
    "### Results and Discussion\n",
    "\n",
    "1. Best-Performing Model<br>\n",
    "    Among the evaluated supervised models (Decision Tree, Naive Bayes, Random Forest), the Random Forest classifier demonstrated the strongest performance. <br>\n",
    "    It achieved the highest overall metrics, including accuracy, F1-score, and ROC-AUC. <br>\n",
    "    This indicates that Random Forest was the most effective model for distinguishing between normal and malicious network flows in the UNSW-NB15 dataset.\n",
    "\n",
    "2. Interpretation of Results and Model Behavior<br>\n",
    "The Decision Tree showed limited generalization and struggled with high-dimensional data, resulting in low recall and overall accuracy.<br>\n",
    "Naive Bayes performed moderately but suffered from poor recall for attacks due to its strong independence assumptions.<br>\n",
    "In contrast, Random Forest handled both numerical and categorical features effectively, capturing complex patterns in the data and maintaining a better balance between precision and recall.<br> \n",
    "Its ensemble structure reduced overfitting and produced the most reliable results across all evaluation metrics.\n",
    "\n",
    "3. Limitations of the Dataset and the Models<br>\n",
    "The UNSW-NB15 dataset contains highly imbalanced classes, skewed numerical distributions, and high-cardinality categorical features, all of which add complexity to model training.<br>\n",
    "Additionally, the binary attack label collapses multiple attack types into one group, making classification less precise.<br>\n",
    "The models also have inherent limitations: Decision Trees overfit easily, Naive Bayes oversimplifies feature relationships, and Random Forests can become computationally expensive and may still misclassify subtle or unseen attack patterns.\n",
    "\n",
    "4. Practical Meaning of the Findings <br>\n",
    "Feature importance analysis from the Random Forest model showed that attributes such as `ct_state_ttl`, `sbytes`, `dbytes`, and rate play major roles in detecting malicious flows.<br>\n",
    "These features correspond to packet behavior, traffic volume, and connection state transitions—factors commonly altered during cyberattacks.<br>\n",
    "This means the trained models can help identify suspicious network activities based on measurable deviations in packet rate, byte count, or TTL values, making them practically useful for intrusion detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d345164",
   "metadata": {},
   "source": [
    "## **Autoencoder for Anomaly Detection**\n",
    "#### **1. Concept of Autoencoder-Based Anomaly Detection**\n",
    "\n",
    "An **autoencoder** is a neural network trained to reconstruct its input.\n",
    "For anomaly detection:\n",
    "- Train the autoencoder only on benign (normal) traffic.\n",
    "- The model learns the normal pattern of network flows.\n",
    "- During testing, malicious traffic produces a high reconstruction error because it differs from normal traffic patterns.\n",
    "- A threshold is set to classify:\n",
    "    - Low error → Normal\n",
    "    - High error → Anomalous (attack)\n",
    "\n",
    "This method is useful because it can detect:\n",
    "- unseen attacks\n",
    "- novel intrusion patterns\n",
    "- deviations from normal behavior\n",
    "\n",
    "This gives an advantage over supervised models, which only detect attacks seen in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b77fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_mask = (y_train == 0)\n",
    "\n",
    "X_benign_encoded = x_train_encoded[benign_mask].copy()\n",
    "X_benign_encoded.columns = X_benign_encoded.columns.astype(str)\n",
    "\n",
    "X_benign_scaled = scaler.transform(X_benign_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "input_dim = X_benign_scaled.shape[1]\n",
    "\n",
    "autoencoder = MLPRegressor(\n",
    "    hidden_layer_sizes=(128, 64, 32, 64, 128),  # optimized symmetric AE\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=80,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(X_benign_scaled, X_benign_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure columns are strings\n",
    "x_test_encoded.columns = x_test_encoded.columns.astype(str)\n",
    "X_test_scaled = scaler.transform(x_test_encoded)\n",
    "\n",
    "# AE reconstruction\n",
    "X_test_reconstructed = autoencoder.predict(X_test_scaled)\n",
    "\n",
    "# Reconstruction error (MSE)\n",
    "mse = np.mean((X_test_scaled - X_test_reconstructed)**2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, mse)\n",
    "youden_index = tpr - fpr\n",
    "best_threshold = thresholds[np.argmax(youden_index)]\n",
    "\n",
    "print(\"Best threshold (Youden):\", best_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ae = (mse > best_threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f910a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "print(\"=== Autoencoder (Optimized) Results ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ae))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_ae))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_ae))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_ae))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, mse))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_ae))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_ae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3137160",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ae = {\n",
    "    \"Model\": \"Autoencoder\",\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred_ae),\n",
    "    \"Precision\": precision_score(y_test, y_pred_ae),\n",
    "    \"Recall\": recall_score(y_test, y_pred_ae),\n",
    "    \"F1-score\": f1_score(y_test, y_pred_ae),\n",
    "    \"ROC-AUC\": roc_auc_score(y_test, mse),\n",
    "}\n",
    "\n",
    "evaluation_df = pd.concat([\n",
    "    evaluation_df,      # your supervised model results\n",
    "    pd.DataFrame([results_ae])\n",
    "], ignore_index=True)\n",
    "\n",
    "evaluation_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa3bfa",
   "metadata": {},
   "source": [
    "#### Best-Performing Supervised Model\n",
    "\n",
    "The Random Forest classifier achieved the best overall performance among the supervised models, with:\n",
    "- Accuracy: 0.703\n",
    "- F1-score: 0.719\n",
    "- ROC-AUC: 0.823\n",
    "- Balanced Accuracy: 0.705\n",
    "\n",
    "This indicates a strong ability to detect malicious traffic while maintaining a reasonable false-positive rate.\n",
    "The tuned Random Forest slightly improved ROC-AUC (0.826), showing better ranking ability, although accuracy decreased marginally due to reduced overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3a534",
   "metadata": {},
   "source": [
    "### **Autoencoder vs Supervised Models**\n",
    "\n",
    "The optimized Autoencoder produced a very different performance profile:\n",
    "- Recall (attack detection): 0.856 - the highest of all models\n",
    "- F1-score: 0.693 (competitive with Random Forest)\n",
    "- Accuracy: 0.582\n",
    "- ROC-AUC: 0.407 (expected for anomaly detection)\n",
    "\n",
    "As an unsupervised anomaly detection model, the Autoencoder is trained only on benign traffic, learning the normal behavior of the network.<br>\n",
    "It flags deviations as anomalies, achieving excellent attack recall but at the cost of more false positives.\n",
    "\n",
    "This makes the Autoencoder particularly useful for:\n",
    "\n",
    "- detecting unknown or novel attack types,\n",
    "- identifying deviations from normal behavior,\n",
    "- complementing supervised classifiers that rely on labeled attack patterns.\n",
    "\n",
    "In contrast, the Random Forest is more accurate overall but may fail to detect unseen or subtle attacks, since it only learns from known labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1685d762",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "#### 1. Random Forest is best for known attacks, offering strong balanced performance.<br>\n",
    "#### 2. Autoencoder is best for unknown or emerging attacks, offering high recall and anomaly sensitivity. <bt>\n",
    "#### 3. Combining both approaches would yield a more robust Intrusion Detection System, where supervised learning handles known threats and the Autoencoder flags unexpected behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
